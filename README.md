# Zentriq â€“ AI Desktop Assistant

Zentriq is an **AI-powered desktop assistant** built using **JavaFX** and **Spring Boot**.  
It integrates **Large Language Models (LLMs)** via **Spring AI (Ollama)** and enhances responses with **Retrieval-Augmented Generation (RAG)** by combining local document context with AI-powered reasoning.

---

## âœ¨ Key Features
- ğŸ–¥ï¸ **Desktop Application** â€“ User-friendly interface built with **JavaFX**.  
- ğŸ¤– **LLM Integration** â€“ Uses **Ollama models** (e.g., `llama3`, `nomic-embed-text`) through **Spring AI**.  
- ğŸ“„ **Document Understanding** â€“ Supports **PDF parsing** (Apache PDFBox) and **text ingestion**.  
- ğŸ” **RAG (Retrieval-Augmented Generation)** â€“ Embeds documents into a **Vector Store** and retrieves relevant context for queries.  
- ğŸ” **Secure & Configurable** â€“ Credentials and model configs handled via Spring profiles.  
- âš¡ **Real-time Chat Interface** â€“ Smooth interaction with AI inside the desktop app.  

---

## ğŸ—ï¸ Architecture Overview

Zentriq combines **desktop UI + backend AI services** in a modular way:

1. **JavaFX Frontend (UI Layer)**  
   - Provides the **chat interface**.  
   - Handles **user input** and displays AI responses.  
   - Implements simple event-driven interactions (send button, enter key).  

2. **Spring Boot Backend (Service Layer)**  
   - Manages AI workflows.  
   - Coordinates between **OllamaChatModel**, **EmbeddingModel**, and **Vector Store**.  
   - Ensures scalable and testable code structure.  

3. **Ollama LLM Integration (AI Layer)**  
   - Connects to a **locally running Ollama server** (`http://localhost:11434`).  
   - Supports **chat models** (`llama3`) for reasoning and **embedding models** (`nomic-embed-text`) for vectorization.  

4. **RAG Pipeline (Knowledge Layer)**  
   - Documents (PDF/TXT) are parsed and indexed into a **Vector Store**.  
   - When a query arrives:  
     - Similar docs are retrieved (`similaritySearch`).  
     - Retrieved context is merged with the query.  
     - Final enriched prompt is passed to the LLM for an accurate, context-aware response.  
